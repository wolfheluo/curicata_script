#!/usr/bin/env python3
import os
import glob
import subprocess
import shutil
import sys
import json
import time
from datetime import datetime, timedelta
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import requests
import geoip2.database
import geoip2.errors
from collections import defaultdict, Counter
import ipaddress




def download_geoip_database():
    """ä¸‹è¼‰ GeoLite2-City è³‡æ–™åº«"""
    print("ğŸ“¡ é–‹å§‹ä¸‹è¼‰ GeoLite2-City è³‡æ–™åº«...")
    
    # MaxMind å…è²»è³‡æ–™åº«çš„ç›´æ¥é€£çµ (éœ€è¦è¨»å†Šæ‰èƒ½å–å¾—)
    # é€™è£¡æä¾›ä¸€å€‹æ›¿ä»£æ–¹æ¡ˆçš„ç¤ºä¾‹
    db_url = "https://github.com/P3TERX/GeoLite.mmdb/raw/download/GeoLite2-City.mmdb"
    
    try:
        response = requests.get(db_url, stream=True)
        response.raise_for_status()
        
        with open('GeoLite2-City.mmdb', 'wb') as f:
            shutil.copyfileobj(response.raw, f)
        
        print("âœ… GeoLite2-City.mmdb ä¸‹è¼‰å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"âŒ ä¸‹è¼‰å¤±æ•—: {e}")
        print("ğŸ’¡ è«‹æ‰‹å‹•ä¸‹è¼‰ GeoLite2-City.mmdb ä¸¦æ”¾ç½®åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„")
        print("   ä¸‹è¼‰ä½ç½®: https://dev.maxmind.com/geoip/geolite2-free-geolocation-data")
        return False


def parse_time_intervals(total_duration_seconds):
    """å°‡ç¸½æ™‚é•·åˆ†å‰²æˆ 10 åˆ†é˜å€é–“"""
    intervals = []
    interval_seconds = 600  # 10 åˆ†é˜
    
    for start in range(0, int(total_duration_seconds) + 1, interval_seconds):
        end = min(start + interval_seconds, total_duration_seconds)
        intervals.append({
            'start_seconds': start,
            'end_seconds': end,
            'duration_minutes': (end - start) / 60
        })
    
    return intervals


def run_tshark_command(tshark_exe, pcap_file, fields, filter_expr=""):
    """åŸ·è¡Œ tshark å‘½ä»¤ä¸¦è¿”å›çµæœ"""
    cmd = [
        tshark_exe,
        "-r", pcap_file,
        "-T", "fields",
        "-E", "separator=|"
    ]
    
    # æ·»åŠ å­—æ®µ
    for field in fields:
        cmd.extend(["-e", field])
    
    # æ·»åŠ éæ¿¾å™¨
    if filter_expr:
        cmd.extend(["-Y", filter_expr])
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8')
        if result.returncode != 0:
            print(f"âš ï¸ tshark è­¦å‘Š: {result.stderr}")
        return result.stdout.strip().split('\n') if result.stdout.strip() else []
    except Exception as e:
        print(f"âŒ åŸ·è¡Œ tshark å‘½ä»¤å¤±æ•—: {e}")
        return []


def analyze_pcap_basic_info(tshark_exe, pcap_file):
    """åˆ†æ PCAP æ–‡ä»¶çš„åŸºæœ¬ä¿¡æ¯ï¼šæ™‚é•·ã€å°åŒ…æ•¸ã€ç¸½æµé‡"""
    print(f"ğŸ“Š åˆ†æåŸºæœ¬ä¿¡æ¯: {os.path.basename(pcap_file)}")
    
    # ç²å–åŸºæœ¬çµ±è¨ˆä¿¡æ¯ï¼ŒåŒ…å«IPå’Œç«¯å£ä¿¡æ¯
    fields = ["frame.time_epoch", "frame.len", "ip.src", "ip.dst", "tcp.srcport", "tcp.dstport", "udp.srcport", "udp.dstport"]
    lines = run_tshark_command(tshark_exe, pcap_file, fields)
    
    if not lines or lines == ['']:
        return None
    
    timestamps = []
    total_bytes = 0
    packet_count = 0
    
    # ç”¨æ–¼å„²å­˜æ¯å€‹10åˆ†é˜å€é–“çš„çµ±è¨ˆ
    per_10_minutes = {}
    per_10_minutes_ip_traffic = {}
    
    for line in lines:
        if '|' in line:
            parts = line.split('|')
            if len(parts) >= 8:
                try:
                    timestamp = float(parts[0])
                    frame_len = int(parts[1])
                    src_ip = parts[2] if parts[2] else ''
                    dst_ip = parts[3] if parts[3] else ''
                    tcp_src_port = parts[4] if parts[4] else ''
                    tcp_dst_port = parts[5] if parts[5] else ''
                    udp_src_port = parts[6] if parts[6] else ''
                    udp_dst_port = parts[7] if parts[7] else ''
                    
                    timestamps.append(timestamp)
                    total_bytes += frame_len
                    packet_count += 1
                    
                    # å°‡æ™‚é–“æˆ³è½‰æ›ç‚º datetime
                    dt = datetime.fromtimestamp(timestamp)
                    
                    # è¨ˆç®—10åˆ†é˜é‚Šç•Œï¼šå°‡åˆ†é˜æ•¸å‘ä¸‹å–æ•´åˆ°10çš„å€æ•¸
                    minute_boundary = (dt.minute // 10) * 10
                    time_key = dt.replace(minute=minute_boundary, second=0, microsecond=0).strftime('%Y-%m-%d %H:%M')
                    
                    # åˆå§‹åŒ–æ™‚é–“å€é–“çµ±è¨ˆ
                    if time_key not in per_10_minutes:
                        per_10_minutes[time_key] = 0
                        per_10_minutes_ip_traffic[time_key] = defaultdict(int)
                    
                    # ç´¯åŠ æ­¤æ™‚é–“å€é–“çš„æµé‡
                    per_10_minutes[time_key] += frame_len
                    
                    # çµ±è¨ˆæ­¤æ™‚é–“å€é–“çš„IPé€£æ¥æµé‡ï¼ˆåŒ…å«ç«¯å£ï¼‰
                    if src_ip and dst_ip:
                        # ç¢ºå®šä½¿ç”¨çš„ç«¯å£
                        src_port = tcp_src_port or udp_src_port or ''
                        dst_port = tcp_dst_port or udp_dst_port or ''
                        
                        if src_port and dst_port:
                            connection = f"{src_ip}:{src_port} -> {dst_ip}:{dst_port}"
                        else:
                            connection = f"{src_ip} -> {dst_ip}"
                        
                        per_10_minutes_ip_traffic[time_key][connection] += frame_len
                    
                except (ValueError, IndexError):
                    continue
    
    if not timestamps:
        return None
    
    start_time = min(timestamps)
    end_time = max(timestamps)
    
    # æŒ‰æ™‚é–“æ’åº per_10_minutes
    sorted_per_10_minutes = dict(sorted(per_10_minutes.items()))
    
    # ç‚ºæ¯å€‹10åˆ†é˜å€é–“ç”Ÿæˆå‰5åIPæµé‡çµ±è¨ˆ
    top_ip_per_10_minutes = {}
    for time_key in sorted(per_10_minutes_ip_traffic.keys()):
        ip_traffic = per_10_minutes_ip_traffic[time_key]
        # æ’åºä¸¦å–å‰5å
        top_connections = sorted(ip_traffic.items(), key=lambda x: x[1], reverse=True)[:5]
        top_ip_per_10_minutes[time_key] = [
            {
                'connection': connection,
                'bytes': bytes_count
            }
            for connection, bytes_count in top_connections
        ]
    
    return {
        'start_time': datetime.fromtimestamp(start_time).isoformat(),
        'end_time': datetime.fromtimestamp(end_time).isoformat(),
        'total_bytes': total_bytes,
        'per_10_minutes': sorted_per_10_minutes,
        'top_ip_per_10_minutes': top_ip_per_10_minutes
    }


def analyze_ip_traffic(tshark_exe, pcap_file):
    """åˆ†æ IP ä¹‹é–“çš„æµé‡ï¼ˆå‰10åï¼ŒåŒ…å« portï¼‰ï¼Œä¸¦è¨˜éŒ„æ¯å€‹é€£æ¥åœ¨ä¸åŒæ™‚é–“æ®µçš„æµé‡"""
    print(f"ğŸŒ åˆ†æ IP æµé‡: {os.path.basename(pcap_file)}")
    
    fields = ["frame.time_epoch", "ip.src", "ip.dst", "tcp.srcport", "tcp.dstport", "udp.srcport", "udp.dstport", "frame.len"]
    lines = run_tshark_command(tshark_exe, pcap_file, fields)
    
    connection_stats = defaultdict(int)
    connection_time_stats = defaultdict(lambda: defaultdict(int))
    
    for line in lines:
        if '|' in line and line.strip():
            parts = line.split('|')
            if len(parts) >= 8:
                try:
                    timestamp = float(parts[0]) if parts[0] else 0
                    src_ip = parts[1] if parts[1] else 'N/A'
                    dst_ip = parts[2] if parts[2] else 'N/A'
                    tcp_src_port = parts[3] if parts[3] else ''
                    tcp_dst_port = parts[4] if parts[4] else ''
                    udp_src_port = parts[5] if parts[5] else ''
                    udp_dst_port = parts[6] if parts[6] else ''
                    frame_len = int(parts[7]) if parts[7] else 0
                    
                    # ç¢ºå®šä½¿ç”¨çš„ç«¯å£
                    src_port = tcp_src_port or udp_src_port or ''
                    dst_port = tcp_dst_port or udp_dst_port or ''
                    
                    if src_ip != 'N/A' and dst_ip != 'N/A' and src_port and dst_port:
                        connection = f"{src_ip}:{src_port} -> {dst_ip}:{dst_port}"
                        connection_stats[connection] += frame_len
                        
                        # è¨ˆç®—10åˆ†é˜æ™‚é–“æ®µ
                        if timestamp > 0:
                            dt = datetime.fromtimestamp(timestamp)
                            minute_boundary = (dt.minute // 10) * 10
                            time_key = dt.replace(minute=minute_boundary, second=0, microsecond=0).strftime('%Y-%m-%d %H:%M')
                            connection_time_stats[connection][time_key] += frame_len
                        
                except (ValueError, IndexError):
                    continue
    
    # æ’åºä¸¦å–å‰10å
    sorted_connections = sorted(connection_stats.items(), key=lambda x: x[1], reverse=True)[:10]
    
    result = []
    for connection, bytes_total in sorted_connections:
        # ç²å–è©²é€£æ¥çš„å‰ä¸‰å€‹æœ€é«˜æµé‡æ™‚é–“æ®µ
        time_stats = connection_time_stats[connection]
        top_time_periods = sorted(time_stats.items(), key=lambda x: x[1], reverse=True)[:3]
        
        # æ ¼å¼åŒ–å‰ä¸‰åæ™‚é–“æ®µè³‡è¨Š
        top_periods_info = []
        for i, (time_period, period_bytes) in enumerate(top_time_periods, 1):
            period_percentage = (period_bytes / bytes_total * 100) if bytes_total > 0 else 0
            top_periods_info.append({
                'rank': i,
                'time_period': time_period,
                'bytes': period_bytes,
                'percentage_of_total': round(period_percentage, 2)
            })
        
        result.append({
            'connection': connection,
            'bytes': bytes_total,
            'top_3_time_periods': top_periods_info
        })
    
    return result


def analyze_protocols(tshark_exe, pcap_file):
    """åˆ†ææ‰€æœ‰å”è­°å‡ºç¾æ¬¡æ•¸å’Œå‰5åé€£æ¥çµ±è¨ˆ"""
    print(f"ğŸ” åˆ†æå”è­°çµ±è¨ˆ: {os.path.basename(pcap_file)}")
    
    # å®šç¾©éœ€è¦è¿½è¹¤çš„å”è­°åˆ—è¡¨
    target_protocols = {
        'DNS', 'DHCP', 'SMTP', 'TCP', 'TLS', 'SNMP', 
        'HTTP', 'FTP', 'SMB3', 'SMB2', 'SMB', 'HTTPS', 'ICMP'
    }
    
    # ç²å–å”è­°çµ±è¨ˆ
    fields = ["frame.protocols", "ip.src", "ip.dst", "frame.len"]
    lines = run_tshark_command(tshark_exe, pcap_file, fields)
    
    protocol_stats = {}
    other_stats = {
        'count': 0,
        'top_ip': '',
        'ip_stats': defaultdict(int),
        'connections': defaultdict(lambda: {'packet_count': 0, 'packet_size': 0})
    }
    
    for line in lines:
        if '|' in line and line.strip():
            parts = line.split('|')
            if len(parts) >= 4:
                try:
                    protocols = parts[0].split(':') if parts[0] else []
                    src_ip = parts[1] if parts[1] else 'N/A'
                    dst_ip = parts[2] if parts[2] else 'N/A'
                    frame_len = int(parts[3]) if parts[3] else 0
                    
                    # æ‰¾å‡ºæœ€é«˜å±¤å”è­°ï¼ˆé€šå¸¸æ˜¯æœ€å¾Œä¸€å€‹ï¼‰
                    main_protocol = None
                    if protocols:
                        # æª¢æŸ¥å”è­°éˆä¸­æ˜¯å¦æœ‰ç›®æ¨™å”è­°ï¼Œå¾å¾Œå¾€å‰æ‰¾ï¼ˆå„ªå…ˆé«˜å±¤å”è­°ï¼‰
                        found_protocol = None
                        for protocol in reversed(protocols):
                            protocol_upper = protocol.upper()
                            if protocol_upper in target_protocols:
                                found_protocol = protocol_upper
                                break
                        
                        # å¦‚æœæ‰¾åˆ°ç›®æ¨™å”è­°ï¼Œä½¿ç”¨å®ƒï¼›å¦å‰‡æ­¸é¡ç‚º other
                        if found_protocol:
                            main_protocol = found_protocol
                        else:
                            main_protocol = 'OTHER'
                        
                        # åˆå§‹åŒ–å”è­°çµ±è¨ˆ
                        if main_protocol != 'OTHER':
                            if main_protocol not in protocol_stats:
                                protocol_stats[main_protocol] = {
                                    'count': 0,
                                    'top_ip': '',
                                    'ip_stats': defaultdict(int),
                                    'connections': defaultdict(lambda: {'packet_count': 0, 'packet_size': 0})
                                }
                            target_stats = protocol_stats[main_protocol]
                        else:
                            target_stats = other_stats
                        
                        target_stats['count'] += 1
                        
                        # çµ±è¨ˆ IP å‡ºç¾æ¬¡æ•¸ï¼Œæ‰¾å‡º top_ip
                        if src_ip != 'N/A':
                            target_stats['ip_stats'][src_ip] += 1
                        if dst_ip != 'N/A':
                            target_stats['ip_stats'][dst_ip] += 1
                        
                        # çµ±è¨ˆé€£æ¥
                        if src_ip != 'N/A' and dst_ip != 'N/A':
                            conn_key = f"{src_ip} -> {dst_ip}"
                            target_stats['connections'][conn_key]['packet_count'] += 1
                            target_stats['connections'][conn_key]['packet_size'] += frame_len
                            
                except (ValueError, IndexError):
                    continue
    
    # å°‡ other çµ±è¨ˆåŠ å…¥çµæœ
    if other_stats['count'] > 0:
        protocol_stats['OTHER'] = other_stats
    
    # æ•´ç†çµæœ
    result = {}
    for protocol, stats in protocol_stats.items():
        # æ‰¾å‡ºå‡ºç¾æœ€å¤šæ¬¡çš„ IP ä½œç‚º top_ip
        top_ip = ''
        if stats['ip_stats']:
            top_ip = max(stats['ip_stats'].items(), key=lambda x: x[1])[0]
        
        # ç²å–å‰5åé€£æ¥
        connections_list = []
        for conn_key, conn_stats in stats['connections'].items():
            src_ip, dst_ip = conn_key.split(' -> ')
            connections_list.append({
                'src_ip': src_ip,
                'dst_ip': dst_ip,
                'packet_count': conn_stats['packet_count'],
                'packet_size': conn_stats['packet_size']
            })
        
        # æŒ‰æµé‡å¤§å°æ’åºå–å‰5å
        connections_list.sort(key=lambda x: x['packet_size'], reverse=True)
        
        result[protocol] = {
            'count': stats['count'],
            'top_ip': top_ip,
            'detailed_stats': connections_list[:5]
        }
    
    return result


def analyze_ip_countries(tshark_exe, pcap_file, geo_reader):
    """çµ±è¨ˆæ‰€æœ‰ IP çš„åœ‹åˆ¥ï¼Œä½¿ç”¨åœ‹å®¶ä»£ç¢¼ä¸¦çµ±è¨ˆæµé‡"""
    print(f"ğŸ—ºï¸ åˆ†æ IP åœ‹åˆ¥: {os.path.basename(pcap_file)}")
    
    fields = ["ip.src", "ip.dst", "frame.len"]
    lines = run_tshark_command(tshark_exe, pcap_file, fields)
    
    country_bytes = defaultdict(int)
    
    for line in lines:
        if '|' in line and line.strip():
            parts = line.split('|')
            if len(parts) >= 3:
                try:
                    src_ip = parts[0] if parts[0] else None
                    dst_ip = parts[1] if parts[1] else None
                    frame_len = int(parts[2]) if parts[2] else 0
                    
                    # è™•ç†ä¾†æº IP
                    if src_ip:
                        country_code = get_country_code(geo_reader, src_ip)
                        if country_code:
                            country_bytes[country_code] += frame_len
                    
                    # è™•ç†ç›®æ¨™ IP
                    if dst_ip:
                        country_code = get_country_code(geo_reader, dst_ip)
                        if country_code:
                            country_bytes[country_code] += frame_len
                            
                except (ValueError, IndexError):
                    continue
    
    # è½‰æ›ç‚ºæ‰€éœ€æ ¼å¼ä¸¦æ’åº
    result = dict(sorted(country_bytes.items(), key=lambda x: x[1], reverse=True))
    
    return result


def get_country_code(geo_reader, ip_address):
    """ç²å– IP åœ°å€çš„åœ‹å®¶ä»£ç¢¼"""
    if not geo_reader or not ip_address:
        return None
    
    try:
        # æª¢æŸ¥æ˜¯å¦ç‚ºç§æœ‰ IP
        ip_obj = ipaddress.ip_address(ip_address)
        if ip_obj.is_private or ip_obj.is_loopback or ip_obj.is_multicast:
            return 'LOCAL'  # æœ¬åœ°ç¶²è·¯çµ±ä¸€ä½¿ç”¨ LOCAL
        
        response = geo_reader.city(ip_address)
        if response.country.iso_code:
            return response.country.iso_code
        else:
            return 'UNKNOWN'
    except (geoip2.errors.AddressNotFoundError, ValueError, Exception):
        return 'UNKNOWN'




def process_pcap_file(pcap_file, out_base, tshark_exe, geo_reader):
    """
    è™•ç†å–®å€‹ PCAP æ–‡ä»¶çš„å‡½æ•¸
    """
    print(f"\nğŸ” é–‹å§‹è™•ç†: {os.path.basename(pcap_file)}")
    
    try:
        # 1. åˆ†æåŸºæœ¬ä¿¡æ¯ï¼ˆç¸½æµé‡ã€ç¸½æ™‚é•·ã€ç¸½å°åŒ…æ•¸ï¼‰
        flow_info = analyze_pcap_basic_info(tshark_exe, pcap_file)
        if not flow_info:
            return f"âŒ ç„¡æ³•åˆ†æ {pcap_file} çš„åŸºæœ¬ä¿¡æ¯"
        
        # 2. åˆ†æ IP æµé‡ (top connections)
        top_connections = analyze_ip_traffic(tshark_exe, pcap_file)
        
        # 3. åˆ†æå”è­°çµ±è¨ˆ (events)
        events = analyze_protocols(tshark_exe, pcap_file)
        
        # 4. åˆ†æ IP åœ‹åˆ¥ (geo)
        geo = analyze_ip_countries(tshark_exe, pcap_file, geo_reader)
        
        # çµ„åˆçµæœç‚ºæ–°æ ¼å¼
        result = {
            'flow': flow_info,
            'top_ip': top_connections,
            'event': events,
            'geo': geo,
            'analysis_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # ä¿å­˜çµæœåˆ° JSON æ–‡ä»¶
        pcap_name = Path(pcap_file).stem
        output_file = os.path.join(out_base, f"{pcap_name}_analysis.json")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… å®Œæˆè™•ç†: {os.path.basename(pcap_file)} -> {output_file}")
        return result
        
    except Exception as e:
        error_msg = f"âŒ è™•ç† {pcap_file} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
        print(error_msg)
        return error_msg


def merge_all_results(results, out_base):
    """åˆä½µæ‰€æœ‰çµæœä¸¦ç”Ÿæˆç¸½çµå ±å‘Š"""
    print("\nğŸ“Š ç”Ÿæˆç¸½çµå ±å‘Š...")
    
    # åˆå§‹åŒ–ç¸½çµæ•¸æ“š
    merged_flow = {
        'start_time': None,
        'end_time': None,
        'total_bytes': 0,
        'per_10_minutes': defaultdict(int),
        'top_ip_per_10_minutes': defaultdict(lambda: defaultdict(int))
    }
    
    merged_top_ip = defaultdict(int)
    merged_top_ip_time_stats = defaultdict(lambda: defaultdict(int))  # æ–°å¢ï¼šåˆä½µæ™‚é–“æ®µçµ±è¨ˆ
    merged_events = {}
    merged_geo = defaultdict(int)
    
    processed_count = 0
    
    for result in results:
        if isinstance(result, dict) and 'flow' in result:
            processed_count += 1
            
            # åˆä½µ flow æ•¸æ“š
            flow = result['flow']
            
            # è¨­å®šé–‹å§‹å’ŒçµæŸæ™‚é–“
            if merged_flow['start_time'] is None or flow['start_time'] < merged_flow['start_time']:
                merged_flow['start_time'] = flow['start_time']
            if merged_flow['end_time'] is None or flow['end_time'] > merged_flow['end_time']:
                merged_flow['end_time'] = flow['end_time']
            
            # ç´¯åŠ ç¸½æµé‡
            merged_flow['total_bytes'] += flow['total_bytes']
            
            # åˆä½µ 10 åˆ†é˜çµ±è¨ˆ
            for time_key, bytes_val in flow['per_10_minutes'].items():
                merged_flow['per_10_minutes'][time_key] += bytes_val
            
            # åˆä½µæ¯å€‹10åˆ†é˜å€é–“çš„å‰5åIPçµ±è¨ˆ
            if 'top_ip_per_10_minutes' in flow:
                for time_key, top_conn_list in flow['top_ip_per_10_minutes'].items():
                    for conn_info in top_conn_list:
                        connection = conn_info['connection']
                        bytes_count = conn_info['bytes']
                        merged_flow['top_ip_per_10_minutes'][time_key][connection] += bytes_count
            
            # åˆä½µ top_ip æ•¸æ“šï¼ˆåŒ…å«æ™‚é–“æ®µçµ±è¨ˆï¼‰
            for conn_info in result['top_ip']:
                connection = conn_info['connection']
                merged_top_ip[connection] += conn_info['bytes']
                
                # åˆä½µæ™‚é–“æ®µçµ±è¨ˆ
                if 'top_3_time_periods' in conn_info:
                    for period_info in conn_info['top_3_time_periods']:
                        time_period = period_info['time_period']
                        period_bytes = period_info['bytes']
                        merged_top_ip_time_stats[connection][time_period] += period_bytes
            
            # åˆä½µ event æ•¸æ“š
            for protocol, protocol_data in result['event'].items():
                if protocol not in merged_events:
                    merged_events[protocol] = {
                        'count': 0,
                        'top_ip': protocol_data['top_ip'],
                        'ip_stats': defaultdict(int),
                        'connections': defaultdict(lambda: {'packet_count': 0, 'packet_size': 0})
                    }
                
                merged_events[protocol]['count'] += protocol_data['count']
                
                # åˆä½µè©³ç´°çµ±è¨ˆ
                for stat in protocol_data['detailed_stats']:
                    conn_key = f"{stat['src_ip']} -> {stat['dst_ip']}"
                    merged_events[protocol]['connections'][conn_key]['packet_count'] += stat['packet_count']
                    merged_events[protocol]['connections'][conn_key]['packet_size'] += stat['packet_size']
            
            # åˆä½µ geo æ•¸æ“š
            for country_code, bytes_val in result['geo'].items():
                merged_geo[country_code] += bytes_val
    
    # æ•´ç†æœ€çµ‚çµæœ
    # Top IP connections (å‰10å) - é‡æ–°è¨ˆç®—å‰ä¸‰åæ™‚é–“æ®µ
    top_connections = []
    for connection, total_bytes in sorted(merged_top_ip.items(), key=lambda x: x[1], reverse=True)[:10]:
        # é‡æ–°è¨ˆç®—è©²é€£æ¥çš„å‰ä¸‰å€‹æœ€é«˜æµé‡æ™‚é–“æ®µ
        time_stats = merged_top_ip_time_stats[connection]
        top_time_periods = sorted(time_stats.items(), key=lambda x: x[1], reverse=True)[:3]
        
        # æ ¼å¼åŒ–å‰ä¸‰åæ™‚é–“æ®µè³‡è¨Š
        top_periods_info = []
        for i, (time_period, period_bytes) in enumerate(top_time_periods, 1):
            period_percentage = (period_bytes / total_bytes * 100) if total_bytes > 0 else 0
            top_periods_info.append({
                'rank': i,
                'time_period': time_period,
                'bytes': period_bytes,
                'percentage_of_total': round(period_percentage, 2)
            })
        
        top_connections.append({
            'connection': connection,
            'bytes': total_bytes,
            'top_3_time_periods': top_periods_info
        })
    
    # Events - é‡æ–°æ•´ç†æ¯å€‹å”è­°çš„å‰5åé€£æ¥
    final_events = {}
    for protocol, data in merged_events.items():
        connections_list = []
        for conn_key, conn_stats in data['connections'].items():
            src_ip, dst_ip = conn_key.split(' -> ')
            connections_list.append({
                'src_ip': src_ip,
                'dst_ip': dst_ip,
                'packet_count': conn_stats['packet_count'],
                'packet_size': conn_stats['packet_size']
            })
        
        # æŒ‰æµé‡å¤§å°æ’åºå–å‰5å
        connections_list.sort(key=lambda x: x['packet_size'], reverse=True)
        
        final_events[protocol] = {
            'count': data['count'],
            'top_ip': data['top_ip'],
            'detailed_stats': connections_list[:5]
        }
    
    # Geo - æŒ‰æµé‡æ’åº
    final_geo = dict(sorted(merged_geo.items(), key=lambda x: x[1], reverse=True))
    
    # è½‰æ› per_10_minutes ç‚ºæ™®é€š dict ä¸¦æŒ‰æ™‚é–“æ’åº
    sorted_per_10_minutes = dict(sorted(merged_flow['per_10_minutes'].items()))
    merged_flow['per_10_minutes'] = sorted_per_10_minutes
    
    # è™•ç†æ¯å€‹10åˆ†é˜å€é–“çš„å‰5åIPçµ±è¨ˆ
    final_top_ip_per_10_minutes = {}
    for time_key in sorted(merged_flow['top_ip_per_10_minutes'].keys()):
        ip_traffic = merged_flow['top_ip_per_10_minutes'][time_key]
        # æ’åºä¸¦å–å‰5å
        top_interval_connections = sorted(ip_traffic.items(), key=lambda x: x[1], reverse=True)[:5]
        final_top_ip_per_10_minutes[time_key] = [
            {
                'connection': connection,
                'bytes': bytes_count
            }
            for connection, bytes_count in top_interval_connections
        ]
    
    merged_flow['top_ip_per_10_minutes'] = final_top_ip_per_10_minutes
    
    total_summary = {
        'summary': {
            'total_files_processed': processed_count,
            'analysis_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        },
        'flow': merged_flow,
        'top_ip': top_connections,
        'event': final_events,
        'geo': final_geo
    }
    
    # ä¿å­˜ç¸½çµå ±å‘Š
    summary_file = os.path.join(out_base, "analysis_summary.json")
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(total_summary, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ç¸½çµå ±å‘Šå·²ä¿å­˜: {summary_file}")
    return total_summary




def main():
    # å¾ç”¨æˆ¶ç²å–ä»£ç¢¼
    code = input("è«‹è¼¸å…¥ä»£ç¢¼: ")
    pcap_dir = input("è«‹è¼¸å…¥ pcap ç›®éŒ„: ")
    
    # è¨­å®šè·¯å¾‘
    tshark_exe = r"C:\Program Files\Wireshark\tshark.exe"
    pcap_dir = pcap_dir.strip()  # å»é™¤é¦–å°¾ç©ºæ ¼
    out_base = os.path.join("project", code)

    # æª¢æŸ¥ GeoIP è³‡æ–™åº«æ˜¯å¦å­˜åœ¨ï¼Œè‹¥ä¸å­˜åœ¨å‰‡ä¸‹è¼‰
    if not os.path.exists('GeoLite2-City.mmdb'):
        download_geoip_database()
    else:
        print("âœ… GeoIP è³‡æ–™åº«å·²å­˜åœ¨")

    # åˆå§‹åŒ– GeoIP è®€å–å™¨
    geo_reader = None
    try:
        geo_reader = geoip2.database.Reader('GeoLite2-City.mmdb')
        print("âœ… GeoIP è³‡æ–™åº«è¼‰å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•è¼‰å…¥ GeoIP è³‡æ–™åº«: {e}")
        print("å°‡è·³éåœ‹åˆ¥åˆ†æåŠŸèƒ½")

    # æª¢æŸ¥ tshark æ˜¯å¦å­˜åœ¨
    if not os.path.exists(tshark_exe):
        print(f"éŒ¯èª¤: æ‰¾ä¸åˆ° tshark åŸ·è¡Œæª” {tshark_exe}")
        return
    
    # æª¢æŸ¥ pcap ç›®éŒ„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(pcap_dir):
        print(f"éŒ¯èª¤: æ‰¾ä¸åˆ° pcap ç›®éŒ„ {pcap_dir}")
        return
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    try:
        os.makedirs(out_base, exist_ok=True)
        print(f"å‰µå»ºè¼¸å‡ºç›®éŒ„: {out_base}")
    except Exception as e:
        print(f"éŒ¯èª¤: ç„¡æ³•å‰µå»ºè¼¸å‡ºç›®éŒ„ {out_base}: {e}")
        return
    
    # å°‹æ‰¾æ‰€æœ‰ .pcap æ–‡ä»¶
    pcap_files = glob.glob(os.path.join(pcap_dir, "*.pcap"))
    
    if not pcap_files:
        print(f"è­¦å‘Š: åœ¨ {pcap_dir} ç›®éŒ„ä¸­æ²’æœ‰æ‰¾åˆ° .pcap æ–‡ä»¶")
        return
    
    print(f"æ‰¾åˆ° {len(pcap_files)} å€‹ PCAP æ–‡ä»¶")
    
    # æ±ºå®šä½¿ç”¨çš„ç·šç¨‹æ•¸é‡
    max_workers = min(8, len(pcap_files)) if len(pcap_files) > 1 else 1

    print(f"\nğŸš€ é–‹å§‹åˆ†æ PCAP æ–‡ä»¶...")
    print(f"ğŸ“‹ åˆ†æé …ç›®:")
    print(f"   1. ç¸½æµé‡ã€ç¸½æ™‚é•·ã€ç¸½å°åŒ…æ•¸ï¼ˆæ¯10åˆ†é˜çµ±è¨ˆï¼‰")
    print(f"   2. IPé–“æµé‡çµ±è¨ˆï¼ˆå‰10åï¼Œå«ç«¯å£ï¼‰")
    print(f"   3. å”è­°çµ±è¨ˆï¼ˆå«å‰5åé€£æ¥ï¼‰")
    print(f"   4. IPåœ‹åˆ¥çµ±è¨ˆï¼ˆä½¿ç”¨GeoLite2ï¼‰")
    print(f"   ğŸ“¤ çµæœå°‡åŒ¯å‡ºç‚ºJSONæ ¼å¼\n")
    
    start_time = time.time()
    
    if max_workers > 1:
        print(f"ä½¿ç”¨ {max_workers} å€‹ç·šç¨‹åŒæ™‚è™•ç† PCAP æ–‡ä»¶...")
        
        # ä½¿ç”¨ç·šç¨‹æ± è™•ç†å¤šå€‹ PCAP æ–‡ä»¶
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»å‹™
            future_to_pcap = {
                executor.submit(process_pcap_file, pcap_file, out_base, tshark_exe, geo_reader): pcap_file 
                for pcap_file in pcap_files
            }
            
            # ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆä¸¦æ”¶é›†çµæœ
            results = []
            completed = 0
            for future in as_completed(future_to_pcap):
                pcap_file = future_to_pcap[future]
                try:
                    result = future.result()
                    results.append(result)
                    completed += 1
                    print(f"é€²åº¦: {completed}/{len(pcap_files)} å®Œæˆ")
                except Exception as exc:
                    error_msg = f"è™•ç† {pcap_file} æ™‚ç™¼ç”Ÿç•°å¸¸: {exc}"
                    print(error_msg)
                    results.append(error_msg)
                    completed += 1
    else:
        print("å–®ç·šç¨‹è™•ç† PCAP æ–‡ä»¶...")
        # å–®ç·šç¨‹è™•ç†
        results = []
        for i, pcap_file in enumerate(pcap_files, 1):
            print(f"é€²åº¦: {i}/{len(pcap_files)}")
            result = process_pcap_file(pcap_file, out_base, tshark_exe, geo_reader)
            results.append(result)
    
    # ç”Ÿæˆç¸½çµå ±å‘Š
    summary = merge_all_results(results, out_base)
    
    end_time = time.time()
    processing_time = end_time - start_time
    
    print(f"\nğŸ‰ åˆ†æå®Œæˆ!")
    print(f"â±ï¸ ç¸½è™•ç†æ™‚é–“: {processing_time:.2f} ç§’")
    print(f"ğŸ“ çµæœä¿å­˜åœ¨: {out_base}")
    print(f"ğŸ“Š è™•ç†äº† {summary['summary']['total_files_processed']} å€‹æ–‡ä»¶")
    print(f" ç¸½æµé‡: {summary['flow']['total_bytes']:,} bytes ({summary['flow']['total_bytes']/1024/1024:.2f} MB)")
    
    # é—œé–‰ GeoIP è®€å–å™¨
    if geo_reader:
        geo_reader.close()


if __name__ == "__main__":
    main()